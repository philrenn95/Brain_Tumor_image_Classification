import numpy as np
import tensorflow as tf
import os
import tensorflow_addons as tfa
import datapipeline
import RESNET
import time
import sklearn.metrics

class ResNetModel:
    def __init__(self, data_dir, batch_size, epochs, img_shape, img_depth):
        """
        Initialisiert das ResNetModel.

        Eingabeparameter:
        - data_dir (str): Pfad zum Verzeichnis der Trainingsdaten.
        - batch_size (int): Größe der Mini-Batches, die während des Trainings verwendet werden.
        - epochs (int): Anzahl der Trainingsepochen.
        - img_shape (int): Höhe und Breite der Bilder (quadratische Bilder werden angenommen).
        - img_depth (int): Anzahl der Kanäle im Bild (z.B. 1 für Graustufenbilder).

        Es wird auch ein optimierter AdaBelief-Optimizer erstellt.
        """
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.epochs = epochs
        self.img_shape = (img_shape, img_shape, img_depth)
        
        # Optimizer mit AdaBelief, einem adaptiven Lernraten-Optimierer.
        self.optimizer = tfa.optimizers.AdaBelief(
            learning_rate=1e-3, 
            warmup_proportion=0.2, 
            min_lr=5e-5, 
            total_steps=self.batch_size
        )

    def train(self):
        """
        Trainiert das Modell.

        Lädt die Daten über den DataPipeline-Klasse, erstellt das Modell und trainiert es über
        eine gegebene Anzahl von Epochen. Pro Epoche werden Trainings- und Validierungsverluste sowie Genauigkeiten berechnet.
        """
        # Erstellen des ResNet-Modells unter Verwendung der Bildform und des Optimizers.
        model = RESNET.building_network(self.img_shape, self.optimizer)
        
        # Initialisieren der Datenpipeline und Laden der Trainings- und Validierungsdaten.
        pipeline = datapipeline.Data_pipeline(directory=self.data_dir, img_shape=self.img_shape)
        x_train, x_validation, y_train, y_validation = pipeline.import_data(train=True)

        for epoch in range(self.epochs):
            epoch_start_time = time.time()

            # Mini-Batch aus den Trainingsdaten erhalten.
            x_minibatch, y_minibatch = self._get_minibatch(x_train, y_train, pipeline)
            # Training des Modells auf dem Mini-Batch.
            train_loss, train_accuracy = model.train_on_batch(x_minibatch, y_minibatch)

            # Mini-Batch aus den Validierungsdaten erhalten.
            x_minibatch, y_minibatch = self._get_minibatch(x_validation, y_validation)
            # Validierung des Modells auf dem Mini-Batch.
            validation_loss, validation_accuracy = model.evaluate(x_minibatch, y_minibatch, verbose=0)

            epoch_end_time = time.time()
            # Ergebnisse des Trainings und der Validierung für diese Epoche ausgeben.
            self._log_epoch_results(epoch, epoch_end_time - epoch_start_time, train_loss, train_accuracy, validation_loss, validation_accuracy)

        # Speichern des trainierten Modells.
        self._save_model(model)

    def test(self):
        """
        Testet das gespeicherte Modell auf Testdaten.

        Lädt das gespeicherte Modell und führt eine Vorhersage auf den Testdaten durch.
        Gibt eine Verwirrungsmatrix (Confusion Matrix) für die Testergebnisse aus.
        """
        print("Testing trained model")
        
        # Laden des gespeicherten Modells.
        model = tf.keras.models.load_model(self._get_model_path())
        
        # Laden der Testdaten über die Datenpipeline.
        pipeline = datapipeline.Data_pipeline(directory=self.data_dir, img_shape=self.img_shape)
        x_test, y_test = pipeline.import_data(test=True)

        # Vorhersage mit dem Modell auf den Testdaten.
        y_pred = model.predict(x_test, verbose=1)
        
        # Verwirrungsmatrix für die wahren und vorhergesagten Werte berechnen.
        confusion_matrix = sklearn.metrics.confusion_matrix(y_true=y_test, y_pred=y_pred)
        print(confusion_matrix)

    def _get_minibatch(self, x_data, y_data, pipeline=None):
        """
        Erzeugt einen Mini-Batch zufällig ausgewählter Daten.

        Eingabeparameter:
        - x_data (np.ndarray): Eingabedaten (z.B. Bilder).
        - y_data (np.ndarray): Zielwerte (Labels).
        - pipeline (Data_pipeline): Optional, eine Datenpipeline für die Datenaugmentation.

        Ausgabewerte:
        - x_minibatch (np.ndarray): Mini-Batch der Eingabedaten.
        - y_minibatch (np.ndarray): Mini-Batch der Zielwerte.
        """
        # Zufällige Indizes für den Mini-Batch auswählen.
        indices = np.random.randint(0, x_data.shape[0], size=self.batch_size)
        x_minibatch = x_data[indices]
        
        # Falls eine Pipeline vorhanden ist, wird die Augmentation angewendet.
        if pipeline:
            x_minibatch = pipeline.augmentaion(x_minibatch)
        
        y_minibatch = y_data[indices]
        return x_minibatch, y_minibatch
